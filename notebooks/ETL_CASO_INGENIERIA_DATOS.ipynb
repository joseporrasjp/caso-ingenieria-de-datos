{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ec1f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo leido correctamente\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "#\n",
    "config_path = \"config/config.yaml\"\n",
    "\n",
    "if not os.path.exists(config_path):\n",
    "    raise FileNotFoundError(f\"no se encontro el file: {config_path}\")\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "print(f\"Archivo leido correctamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608ae458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, current_timestamp\n",
    "from pyspark.sql.types import IntegerType, DecimalType\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "def get_data(spark, config):\n",
    "    #salida fecha proceso \n",
    "    #(output_path: data/processed/${fecha_proceso})\n",
    "    df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(config.input.file_path)\n",
    "    df = df.withColumn(\"fecha_proceso\", col(\"fecha_proceso\").cast(\"string\"))\n",
    "    \n",
    "    return df.filter(\n",
    "        (col(\"fecha_proceso\") >= config.filters.start_date) & \n",
    "        (col(\"fecha_proceso\") <= config.filters.end_date) & \n",
    "        (col(\"pais\") == config.filters.country)\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_data(df, config):\n",
    "\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"cantidad_unidades\",\n",
    "        when(\n",
    "            col(\"unidad\") == \"CS\",\n",
    "            col(\"cantidad\") * config.processing.box_size #llamamos al parametro box_size del config.yaml que tiene como valor 20 \n",
    "        ).when(\n",
    "            col(\"unidad\") == \"ST\",\n",
    "            col(\"cantidad\")\n",
    "        ).otherwise(None) # Default null si no coincide ninguna unidad\n",
    "    )\n",
    "\n",
    "\n",
    "    routine_types = list(config.processing.delivery_types.routine)\n",
    "    bonus_types = list(config.processing.delivery_types.bonus)\n",
    "    all_types = routine_types + bonus_types\n",
    "\n",
    "\n",
    "    df = df.filter(\n",
    "        col(\"tipo_entrega\").isin(all_types)\n",
    "    )\n",
    "\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\n",
    "            \"entrega_rutina\",\n",
    "            col(\"tipo_entrega\").isin(routine_types)\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"entrega_bonificacion\",\n",
    "            col(\"tipo_entrega\").isin(bonus_types)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"precio\",\n",
    "        when(col(\"precio\") > 0, col(\"precio\"))\n",
    "        .cast(DecimalType(10, 2))\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .dropna(subset=[\"cantidad_unidades\"])\n",
    "        .filter(col(\"cantidad_unidades\") > 0)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# Carga de datos\n",
    "def load_data(df, config):\n",
    "    #se estandarizaron los nombres de paises\n",
    "    country_map = {\n",
    "        \"GT\": \"Guatemala\", \n",
    "        \"PE\": \"Peru\", \n",
    "        \"EC\": \"Ecuador\", \n",
    "        \"SV\": \"El Salvador\", \n",
    "        \"HN\": \"Honduras\", \n",
    "        \"JM\": \"Jamaica\"\n",
    "        }\n",
    "    \n",
    "    df = df.replace(country_map, subset=[\"pais\"])\n",
    "    \n",
    "        #Estandar de nombres de columnas\n",
    "    output_df = df.select(\n",
    "        col(\"pais\").alias(\"country\"),\n",
    "        col(\"material\"),\n",
    "        col(\"transporte\").alias(\"transport\"),\n",
    "        col(\"ruta\").alias(\"route\"),\n",
    "        col(\"precio\").alias(\"price\"),\n",
    "        col(\"cantidad_unidades\").cast(IntegerType()).alias(\"unit_quantity\"),\n",
    "        col(\"entrega_rutina\").alias(\"routine_delivery\"),\n",
    "        col(\"entrega_bonificacion\").alias(\"bonus_delivery\"),\n",
    "        col(\"fecha_proceso\"), \n",
    "        current_timestamp().alias(\"load_date\")\n",
    "    )\n",
    "    \n",
    "    output_path = config.output.base_path\n",
    "    if os.path.exists(output_path):\n",
    "        shutil.rmtree(output_path)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    pdf = output_df.toPandas() #se utilizo una conversión a Pandas para la ejecución  local en Windows sin dependencias de binarios de Hadoop (winutils.exe).\n",
    "    \n",
    "    pdf.to_parquet(output_path, engine='pyarrow', compression='snappy', partition_cols=['fecha_proceso'], index=False)\n",
    "    \n",
    "    #CSV\n",
    "    for root, _, files in os.walk(output_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".parquet\"):\n",
    "                parquet_file = os.path.join(root, file)\n",
    "                csv_file = parquet_file.replace(\".parquet\", \".csv\")\n",
    "                pd.read_parquet(parquet_file).to_csv(csv_file, index=False)\n",
    "                #print(f\"Generado: {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31172e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros encontrados: 12\n",
      "--- Fin del Proceso Exitoso ---\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_CASO_INGENIERIA_DATOS\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "try:\n",
    "    # obtener datos\n",
    "    df_raw = get_data(spark, config)\n",
    "    \n",
    "    if df_raw.count() == 0:\n",
    "        print(\"No se encontraron registros que cumplan con los filtros\")\n",
    "    else:\n",
    "        print(f\"Registros encontrados: {df_raw.count()}\")\n",
    "\n",
    "    #transformacion de la data y normalizacion\n",
    "        df_processed = transform_data(df_raw, config)\n",
    "        #df_processed.show(5)\n",
    "        \n",
    "    #Carga de los datos\n",
    "        load_data(df_processed, config)\n",
    "        \n",
    "    print(\"--- Fin del Proceso Exitoso ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la ejecucion: {e}\")\n",
    "\n",
    "finally:\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
